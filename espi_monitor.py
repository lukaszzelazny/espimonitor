import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import os
from dotenv import load_dotenv
import logging
import hashlib


class ESPIMonitor:
    def __init__(self):
        # Za≈Çaduj zmienne ≈õrodowiskowe
        load_dotenv()

        # Konfiguracja logowania
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('espi_monitor.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        # URL strony ESPI
        self.url = "https://espiebi.pap.pl/"

        # Obserwowane sp√≥≈Çki z pliku .env
        watched_companies_str = os.getenv('WATCHED_COMPANIES', '')
        self.watched_companies = [company.strip().upper() for company in
                                  watched_companies_str.split(',') if company.strip()]

        if not self.watched_companies:
            self.logger.warning(
                "Brak obserwowanych sp√≥≈Çek w pliku .env. Dodaj WATCHED_COMPANIES=Dekpol,InnaSp√≥≈Çka")

        # Przechowywanie hashy poprzednich wpis√≥w dla wykrywania nowych
        self.previous_entries = set()

        # Konfiguracja sesji HTTP
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        self.logger.info(
            f"Monitor ESPI uruchomiony. Obserwowane sp√≥≈Çki: {', '.join(self.watched_companies)}")

    def fetch_page(self):
        """Pobiera zawarto≈õƒá strony ESPI"""
        try:
            response = self.session.get(self.url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            self.logger.error(f"B≈ÇƒÖd podczas pobierania strony: {e}")
            return None

    def parse_entries(self, html_content):
        """Parsuje HTML i wyciƒÖga wpisy ESPI"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # Szukamy kontener√≥w z wpisami ESPI
            # Na podstawie struktury strony, wpisy sƒÖ prawdopodobnie w divach lub li
            entries = []

            # Pr√≥bujemy r√≥≈ºne selektory w zale≈ºno≈õci od struktury strony
            possible_selectors = [
                '.espi-item', '.espi-entry', '.report-item',
                'div[class*="espi"]', 'li[class*="espi"]',
                'tr', 'div.item', '.news-item'
            ]

            for selector in possible_selectors:
                elements = soup.select(selector)
                if elements:
                    self.logger.info(f"Znaleziono elementy z selektorem: {selector}")
                    break

            # Je≈õli nie znajdziemy specyficznych selektor√≥w, szukamy link√≥w
            if not elements:
                # Szukamy wszystkich link√≥w, kt√≥re mogƒÖ prowadziƒá do raport√≥w
                elements = soup.find_all('a', href=True)

            for element in elements:
                try:
                    # WyciƒÖgnij tytu≈Ç
                    title = ""
                    if element.get_text(strip=True):
                        title = element.get_text(strip=True)
                    elif element.get('title'):
                        title = element.get('title')

                    # WyciƒÖgnij link
                    link = ""
                    if element.name == 'a':
                        link = element.get('href', '')
                    else:
                        # Szukaj linka wewnƒÖtrz elementu
                        link_elem = element.find('a', href=True)
                        if link_elem:
                            link = link_elem.get('href', '')
                            if not title:
                                title = link_elem.get_text(strip=True)

                    # Uzupe≈Çnij wzglƒôdny link do pe≈Çnego URL
                    if link and not link.startswith('http'):
                        if link.startswith('/'):
                            link = 'https://espiebi.pap.pl' + link
                        else:
                            link = 'https://espiebi.pap.pl/' + link

                    # Dodaj wpis je≈õli ma tytu≈Ç i link
                    if title and link and len(
                            title) > 10:  # Filtruj bardzo kr√≥tkie teksty
                        entries.append({
                            'title': title,
                            'link': link
                        })

                except Exception as e:
                    self.logger.debug(f"B≈ÇƒÖd podczas parsowania elementu: {e}")
                    continue

            self.logger.info(f"Znaleziono {len(entries)} wpis√≥w")
            return entries[:20]  # Ograniczamy do 20 najnowszych wpis√≥w

        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd podczas parsowania HTML: {e}")
            return []

    def check_company_match(self, title):
        """Sprawdza czy tytu≈Ç zawiera nazwƒô obserwowanej sp√≥≈Çki"""
        title_upper = title.upper()

        for company in self.watched_companies:
            if company in title_upper:
                return company
        return None

    def generate_entry_hash(self, entry):
        """Generuje hash wpisu do wykrywania duplikat√≥w"""
        content = f"{entry['title']}{entry['link']}"
        return hashlib.md5(content.encode()).hexdigest()

    def process_entries(self, entries):
        """Przetwarza wpisy i sprawdza czy sƒÖ nowe oraz czy dotyczƒÖ obserwowanych sp√≥≈Çek"""
        new_matches = []
        current_hashes = set()

        for entry in entries:
            entry_hash = self.generate_entry_hash(entry)
            current_hashes.add(entry_hash)

            # Sprawd≈∫ czy to nowy wpis
            if entry_hash not in self.previous_entries:
                # Sprawd≈∫ czy dotyczy obserwowanej sp√≥≈Çki
                matched_company = self.check_company_match(entry['title'])
                if matched_company:
                    new_matches.append({
                        'company': matched_company,
                        'title': entry['title'],
                        'link': entry['link']
                    })

        # Aktualizuj zestaw poprzednich wpis√≥w
        self.previous_entries = current_hashes

        return new_matches

    def display_matches(self, matches):
        """Wy≈õwietla znalezione dopasowania w konsoli"""
        for match in matches:
            print("\n" + "=" * 80)
            print(f"üö® NOWY RAPORT ESPI - {match['company']}")
            print(f"üìã Tytu≈Ç: {match['title']}")
            print(f"üîó Link: {match['link']}")
            print(f"‚è∞ Czas: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 80 + "\n")

    def run_once(self):
        """Jednorazowe sprawdzenie strony"""
        self.logger.info("Rozpoczynam sprawdzanie strony ESPI...")

        html_content = self.fetch_page()
        if not html_content:
            return

        entries = self.parse_entries(html_content)
        if not entries:
            self.logger.warning(
                "Nie znaleziono ≈ºadnych wpis√≥w. Mo≈ºliwe, ≈ºe struktura strony siƒô zmieni≈Ça.")
            return

        matches = self.process_entries(entries)

        if matches:
            self.display_matches(matches)
            self.logger.info(
                f"Znaleziono {len(matches)} nowych raport√≥w dla obserwowanych sp√≥≈Çek")
        else:
            self.logger.info("Brak nowych raport√≥w dla obserwowanych sp√≥≈Çek")

    def run(self):
        """G≈Ç√≥wna pƒôtla monitorowania"""
        self.logger.info("Uruchamiam monitor ESPI. Naci≈õnij Ctrl+C aby zatrzymaƒá.")

        # Pierwsze uruchomienie - ≈Çadujemy aktualne wpisy bez powiadomie≈Ñ
        html_content = self.fetch_page()
        if html_content:
            entries = self.parse_entries(html_content)

            ####
            matches = self.process_entries(entries)
            if matches:
                self.display_matches(matches)

            #####

            for entry in entries:
                entry_hash = self.generate_entry_hash(entry)
                self.previous_entries.add(entry_hash)
            self.logger.info("Za≈Çadowano aktualne wpisy jako punkt odniesienia")

        try:
            while True:
                self.run_once()
                self.logger.info("Czekam 60 sekund do nastƒôpnego sprawdzenia...")
                time.sleep(60)  # Czekaj minutƒô

        except KeyboardInterrupt:
            self.logger.info("Monitor zatrzymany przez u≈ºytkownika")
        except Exception as e:
            self.logger.error(f"Nieoczekiwany b≈ÇƒÖd: {e}")

    def test_entry(self, title, link):
        """Testowa metoda do sprawdzenia konkretnego wpisu"""
        print(f"\nüß™ TEST WPISU:")
        print(f"Tytu≈Ç: {title}")
        print(f"Link: {link}")
        print(f"Obserwowane sp√≥≈Çki: {self.watched_companies}")

        matched_company = self.check_company_match(title)
        if matched_company:
            print(f"‚úÖ DOPASOWANIE: {matched_company}")
            self.display_matches([{
                'company': matched_company,
                'title': title,
                'link': link
            }])
        else:
            print("‚ùå Brak dopasowania")
        print()


if __name__ == "__main__":
    monitor = ESPIMonitor()
    monitor.run()