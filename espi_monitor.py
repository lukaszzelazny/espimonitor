import requests
from bs4 import BeautifulSoup
from openai import OpenAI

import time
from datetime import datetime
import os
from dotenv import load_dotenv
import logging
import hashlib
import re

SECTION_END_MARKERS = [
    "Za≈ÇƒÖczniki",
    "MESSAGE (ENGLISH VERSION)",
    "INFORMACJE O PODMIOCIE",
    "PODPISY OS√ìB REPREZENTUJƒÑCYCH SP√ì≈ÅKƒò",
    "PODPISY",
]

class ESPIMonitor:
    model = "gpt-4o-mini"
    system_prompt = """
    Jeste≈õ analitykiem gie≈Çdowym. Twoim zadaniem jest oceniƒá komunikat gie≈Çdowy (ESPI). 
    Ocenƒô wyra≈ºasz jako liczbƒô ca≈ÇkowitƒÖ od -5 do 5:
    -5 oznacza bardzo negatywny wp≈Çyw na kurs akcji,
    0 oznacza neutralny,
    5 oznacza bardzo pozytywny (np. nowe kontrakty, znaczƒÖce zyski).  

    Odpowiadaj wy≈ÇƒÖcznie w formacie JSON: 
    {
      "ocena": <liczba od -5 do 5>,
      "uzasadnienie": "<kr√≥tkie uzasadnienie oceny>"
    }
    """
    def __init__(self):
        # Za≈Çaduj zmienne ≈õrodowiskowe
        load_dotenv()

        # Konfiguracja logowania - proste bez rotacji
        # Ale tylko INFO i wy≈ºsze poziomy (bez DEBUG)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('espi_monitor.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        # URL strony ESPI
        self.url = "https://espiebi.pap.pl/"
        self.client = OpenAI(api_key=os.getenv('OPENAI_API', ''))

        # Obserwowane sp√≥≈Çki z pliku .env
        watched_companies_str = os.getenv('WATCHED_COMPANIES', '')
        self.watched_companies = [company.strip().upper() for company in
                                  watched_companies_str.split(',') if company.strip()]

        if not self.watched_companies:
            self.logger.warning(
                "Brak obserwowanych sp√≥≈Çek w pliku .env. Dodaj WATCHED_COMPANIES=Dekpol,InnaSp√≥≈Çka")

        # Przechowywanie hashy poprzednich wpis√≥w
        self.previous_entries = set()

        # Konfiguracja sesji HTTP
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        self.logger.info(
            f"Monitor ESPI uruchomiony. Obserwowane sp√≥≈Çki: {', '.join(self.watched_companies)}")

    def fetch_page(self):
        """Pobiera zawarto≈õƒá strony ESPI"""
        try:
            response = self.session.get(self.url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            self.logger.error(f"B≈ÇƒÖd podczas pobierania strony: {e}")
            return None

    def pobierz_komunikat_espiebi(self, url: str) -> dict:
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        full_text = soup.get_text("\n", strip=True)

        # --- TEMAT ---
        temat = None
        m = re.search(r"Temat\s*[:\-]?\s*(.+)", full_text)
        if m:
            temat = m.group(1).strip()
        else:
            h1 = soup.find("h1")
            if h1:
                temat = h1.get_text(strip=True)

        # --- TRE≈öƒÜ ---
        tresc = None
        m = re.search(r"Tre≈õƒá raportu:\s*(.+)", full_text, re.S)
        if m:
            tresc = m.group(1).strip()
            # przytnij na pierwszym markerze
            for marker in SECTION_END_MARKERS:
                idx = tresc.find(marker)
                if idx != -1:
                    tresc = tresc[:idx].strip()
                    break

        return {"temat": temat, "tresc": tresc}

    def parse_entries(self, html_content):
        """Parsuje HTML i wyciƒÖga wpisy ESPI"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            entries = []

            # Szukamy element√≥w li.news (struktura ESPI)
            news_items = soup.find_all('li', class_='news')

            self.logger.info(f"Znaleziono {len(news_items)} element√≥w li.news")

            for item in news_items:
                try:
                    # Znajd≈∫ link
                    link_elem = item.find('a', class_='link')
                    if not link_elem:
                        continue

                    title = link_elem.get_text(strip=True)
                    link = link_elem.get('href', '')

                    # Znajd≈∫ wszystkie div.hour dla daty
                    hour_divs = item.find_all('div', class_='hour')

                    # WyciƒÖgnij czas z pierwszego div.hour i datƒô/numer z drugiego
                    time_str = ""
                    date_info = ""

                    if len(hour_divs) >= 1:
                        time_str = hour_divs[0].get_text(strip=True)
                    if len(hour_divs) >= 2:
                        date_info = hour_divs[1].get_text(strip=True)

                    # Stw√≥rz datƒô - u≈ºyj dzisiejszej daty + czas z ESPI
                    if time_str and re.match(r'\d{1,2}:\d{2}', time_str):
                        today = datetime.now().strftime('%Y-%m-%d')
                        full_date = f"{today} {time_str}"
                    else:
                        full_date = "Nie znaleziono daty"

                    # Uzupe≈Çnij link
                    if link and not link.startswith('http'):
                        if link.startswith('/'):
                            link = 'https://espiebi.pap.pl' + link
                        else:
                            link = 'https://espiebi.pap.pl/' + link

                    dane = self.pobierz_komunikat_espiebi(link)

                    if title and link:
                        entries.append({
                            'title': title,
                            'link': link,
                            'date': full_date,
                            'time_raw': time_str,
                            'date_info_raw': date_info,
                            'report': dane['temat'],
                            'details': dane['tresc']
                        })

                except Exception as e:
                    self.logger.debug(f"B≈ÇƒÖd parsowania elementu: {e}")
                    continue

            self.logger.info(f"Sparsowano {len(entries)} wpis√≥w")
            return entries

        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd podczas parsowania HTML: {e}")
            return []

    def check_company_match(self, title):
        """Sprawdza czy tytu≈Ç zawiera nazwƒô obserwowanej sp√≥≈Çki"""
        title_upper = title.upper()

        for company in self.watched_companies:
            company_upper = company.upper()
            if company_upper in title_upper:
                return company
        return None

    def generate_entry_hash(self, entry):
        """Generuje hash wpisu"""
        content = f"{entry['title']}{entry['link']}"
        return hashlib.md5(content.encode()).hexdigest()

    def process_entries(self, entries):
        """Przetwarza wpisy i sprawdza nowe oraz obserwowane sp√≥≈Çki"""
        new_matches = []
        current_hashes = set()

        for entry in entries:
            entry_hash = self.generate_entry_hash(entry)
            current_hashes.add(entry_hash)

            # Sprawd≈∫ czy nowy wpis
            is_new = entry_hash not in self.previous_entries

            if is_new:
                # Sprawd≈∫ czy dotyczy obserwowanej sp√≥≈Çki
                matched_company = self.check_company_match(entry['title'])
                if matched_company:
                    new_matches.append({
                        'company': matched_company,
                        'title': entry['title'],
                        'link': entry['link'],
                        'date': entry['date'],
                        'report': entry['report'],
                        'details': entry['details']
                    })

        # Aktualizuj poprzednie wpisy
        self.previous_entries = current_hashes
        return new_matches

    def display_matches(self, matches):

        """Wy≈õwietla dopasowania w konsoli"""
        for match in matches:
            temat = match['report']
            tresc = match['details']
            completion = self.client.chat.completions.create(
                model=self.model,
                messages = [
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": f"Temat: {temat}\nTre≈õƒá: {tresc}"}
                ]
            )


            print("\n" + "=" * 80)
            print(f"üö® NOWY RAPORT ESPI - {match['company']}")
            print(f"üìã Nag≈Ç√≥wek: {match['title']}")
            print(f"üîó Link: {match['link']}")
            print(f"üìÖ Data ESPI: {match['date']}")
            print(f"üìã Temat: {match['report']}")
            print(f"üìã Tre≈õƒá: {match['details']}")
            print(f"‚è∞ Wykryto: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"üìã OCENA AI: {completion.choices[0].message.content}")
            print("=" * 80 + "\n")

    def run_once(self):
        """Jednorazowe sprawdzenie"""
        self.logger.info("Sprawdzam stronƒô ESPI...")

        html_content = self.fetch_page()
        if not html_content:
            return

        entries = self.parse_entries(html_content)
        if not entries:
            self.logger.warning("Nie znaleziono wpis√≥w")
            return

        matches = self.process_entries(entries)

        if matches:
            self.display_matches(matches)
            self.logger.info(f"Znaleziono {len(matches)} nowych raport√≥w")
        else:
            self.logger.info("Brak nowych raport√≥w dla obserwowanych sp√≥≈Çek")

    def run(self):
        """G≈Ç√≥wna pƒôtla monitorowania"""
        self.logger.info("Uruchamiam monitor ESPI. Ctrl+C aby zatrzymaƒá.")

        # Pierwsze uruchomienie - za≈Çaduj obecne wpisy
        html_content = self.fetch_page()
        if html_content:
            entries = self.parse_entries(html_content)

            matches = self.process_entries(entries)
            if matches:
                self.display_matches(matches)

            for entry in entries:
                entry_hash = self.generate_entry_hash(entry)
                self.previous_entries.add(entry_hash)
            self.logger.info("Za≈Çadowano istniejƒÖce wpisy")

        try:
            while True:
                self.run_once()
                self.logger.info("Czekam 60 sekund...")
                time.sleep(60)
        except KeyboardInterrupt:
            self.logger.info("Monitor zatrzymany")
        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd: {e}")


if __name__ == "__main__":
    monitor = ESPIMonitor()
    monitor.run()